In this blog post, I attempt to explore the use of recurrent neural networks (RNNs) in predicting 
what items *Invoker* players will purchase in standard *DotA 2* matches. 
*DotA 2* is a game managed by Valve (and lead developed by IceFrog, hallowed be his name) with team-based,
player versus player, and role-playing elements. As with most role-playing games, there exists a diegetic component 
of a character build where a player must choose how the character he or she plays is developed. One of these elements of construction
includes what items a player will purchase with in-game rewards in order to strengthen their character, and I attempt to predict 
this particular element with RNNs. The problem was limited to one of the characters of *DotA 2* in higher level matches.
This blog post will not focus on particularly difficult techniques; the network is actually a simple LSTM cell, and the code
runs on most computers within the past 5 years. 

All of the code referenced in this post---including data gathering, pre-processing, and modeling---are 
included in [this project](https://github.com/beelzebud/Dota-ItemSequence). The frameworks used include NumPy, SciPy and TensorFlow. 
Special thanks goes out to the [Stratz team](https://stratz.com) for their assistance in obtaining data using their API.

So why even predict what items a player will buy as Invoker? This could be used as a tool to teach new players how to emulate higher
level players. Professional players could explore lots of potential builds relatively quickly in contextual scenarios (although this
model does not take into account the general state of a match). And if it works for items, it could reasonably work for skill progression
or hero drafting.

Recurrent neural networks are particularly useful in supervised learning environments where sequence and order matters such as natural
language processing, audio and visual encodings, and stateful changes. *DotA 2* is a game with a lot of state, and I only use a limited
number of features of this state, but the results seem initially promising.
For this particular project, the network consists of a simple long short term memory cell. 

The data was obtained by using the [Stratz API](https://docs.stratz.com). 
I was given a set of matches with a high level of skill by Ken of Stratz. For the players in these games, I then sought out their
recent Invoker matches in patch 7.06e. Matches were limited to ranked All Pick or Captain's mode games. For these games, I obtained
their list of purchased items, and this was the data actually used in the model.
There is additional data obtained that are usable as features, but I did not incorporate them this iteration. This includes past 
performance of allies and enemies on the hero of their choice. For example, if a teammate played Crystal in a particular game,
I also gathered data for how that player did as Crystal Maiden in the previous 50 games in terms of the average kills, deaths, and other
relevant pieces of information. Nonetheless, this extra information was not used in this model and was left as something of interest.

# How does data transit through the network?
To cover the motivations and mechanics of RNNs in this blog post would be ambitious, but I hope it will suffice to qualify the process
of how an LSTM cell predicts the next item given the previous item. This will be a very high level overview, so some of the terms will
be non-standard. The network consists of the input layer (the item), a long short term
memory (LSTM) cell, and the output of that cell. To make the items interpretable by the architecture of the neural network, each item has
its item identifier number converted to 266 different bits of information 
(the number of purchasable items, including the possibility of no item). 
So the *Infused Raindrops* with an item ID of 265 will have 265
bits marked as zero, but the 266th bit will be denoted as one; the first item bit is reserved for padding to denote no item purchase and
the ending of a sequence in a game. These 266th different bits of information, henceforth called neurons, compose the input layer. The
neurons then send their information into the LSTM cell. The LSTM remembers its previous output and its previous state. The previous output
serves the function of being a short term memory for the cell, and the previous state serves as a long term memory for the cell. 
How does the network deal with the first prediction? The initial short term memory and long term memory are empty, and I always started
with a few initial items to seed the item sequence generation process.

The cell uses the short term memory and the current input to learn what to forget from the long term memory with use of 
forgetting parameters. It also uses the short term memory and input along with remembering parameters to know what should enter the
long term memory. Some of the forgetting and remembering parameters along with the old long term memory and incoming long term memory
are used to generate a new long term memory (or a new state for the cell). 

With this new state, the cell can generate output. A set of output parameters generates an output layer using the
input layer, short term memory (the previous output), and the new long term memory (the new state). Finally, I map the output layer of
the cell to a final layer consisting of 266 neurons using a final set of parameters. These neurons are all activated in such a way
that their values are between 0 and 1. Think of this as the neuron stating how confident that the next item will be the one corresponding
to that bit. 
The neuron with the highest activation denotes what the next predicted item should be. So if the third bit was the bit with the value
closest to 1, then the item with the ID "2" (Blades of Attack) would be the predicted next item.


# How does it learn to do this?
Given the matches that I collected, I partition this data into completely distinct training and test sets. The training set is used to 
teach the network how the parameters should be tweaked. The way it does that is that it tries to do a bunch of predictions based
on the transitions of purchased item to the next purchased item for the item sequences (the different matches). If the prediction is 
wrong, then it tweaks the necessary parameters. The training process goes through many iterations of failed predictions and tweaks.
If you want more details, look up (truncated) backpropagation through time. Some knowledge in linear algebra and basic calculus will help.

However, one must be careful to not use the training set to determine how good the model is. That is why we test on the test set.
Sometimes, there is a validation set to evaluate the performance of different models. The model that performs best on the validation set
is then used to evaluate on the test set to determine the final score for the model. However, I only made one model, and there wasn't 
enough data to make all three different sets.

# Some Results

One thing to keep in mind is that randomly guessing would have an average accuracy of about 0.4% over an entire item sequence.
My model had an accuracy of about 55% on the test sequences with a standard deviation of 9%. It took about two hours to train 
on a three year old laptop. 

![](/static/graphs/ItemSequencing/accuracy.png)

*Here, I use TensorBoard to monitor how the accuracy of the model changes through iterations of training. You can use TensorBoard to
see if there are any funky things going on with your model during training.*

![](/static/graphs/ItemSequencing/loss.png)

*Similar graph as above, but I display the loss instead. The loss here was defined to be the cross-entropy between the target item
and the predicted items. In a sense, this measures how disjoint the distributions were. As training goes on, the model performs better
on the training set.*

Here is part of an example sequence that the model generates when using a circlet, mantle, and null talisman recipe as the initial items:

- branches
- flask
- circlet
- mantle
- recipe_null_talisman
- null_talisman
- boots
- gloves
- tpscroll
- recipe_hand_of_midas
- hand_of_midas
- tpscroll
- tpscroll
- point_booster
- staff_of_wizardry
- ogre_axe
- blade_of_alacrity
- ultimate_scepter
- tpscroll
- recipe_travel_boots
- travel_boots
- blink

Here is a list of matches and their corresponding sequence accuracy (corresponding to top 3 and bottom 3 accuracies):

match id|accuracy
:--------:|:--------:
3293379090|0.762
3301460616|0.744
3308683837|0.738
3309468461|0.25
3306228851|0.25
3305013972|0.091

# Possible Future Work

One thing I could have done is done is some processing on the sequences. 
For example, you cannot buy a Null Talisman, but
you instead buy a circlet, mantle, and recipe in some order. Better grouping of items and mapping such as some
combination of the Null talisman components into an indication of buying a Null Talisman could extract more
meaningful signal. After all, it doesn't matter what order you buy components of an item. However, I did not want
to deal with situations with when people opt to not finish an item immediately. I could have also removed
Null Talisman or any item that isn't a purchaseable item and limited the number of targets.

I also forgot to take out from the sequences items that were context-dependent for buying, 
such as teleport scrolls or salves.

Lastly, I did not train on enough data. There were only about 1000 matches in this data set, which is arguably not enough given how
dismal or "predictable" the predictions could be. The model can also be a lot more complex such as stacking cells.

# Conclusion
RNNs seem to fit the right bill to predict Invoker items in most typical matches. This could easily be a tool to help new players 
learn item progression. If the model were to be built with consideration of the unused features such as the past performance of allies
and enemies on the hero they have chosen, then the suggestions can become much more contextualized. The model and data themselves could
be better refined. Nonetheless, the model does much better than random chance. Perhaps the same idea could be applied to hero drafting
or skill progression. 

If anybody wants to further build on this or just use the data, feel free to comment or just go straight to the Github! Thanks for reading.

# References
- [Some official TensorFlow documentation of RNNs](https://www.tensorflow.org/tutorials/recurrent)
- [colah's excellent article on LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Some practical features of the basic RNN cells and their wrapping libraries highlighted by Denny Britz]
(http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
- [Danijar's article on variable length sequences in TensorFlow]
(https://danijar.com/variable-sequence-lengths-in-tensorflow/)
- [Another article on variable length sequences]
(https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html)
