Hi, in this blog post, I hope to explore the performance of neural networks, particular auto-encoders, in detecting anomalous DotA matches 
using the feature pipeline (if it can even be called such) I have published on my github. 
Essentially, the idea was to collect a bunch of matches from Patch 7.06c and then feed them into a neural network to detect 
*weird* matches. 

To give a bit of a context, **DotA 2** is a an online video game managed by *Valve* 
The matches in the designated training and test sets would be used train the neural network and subsequently generate a distribution of
reconstruction errors. If the autoencoder and the features used were both perfect, then the matches would be reconstructed perfectly with no 
error; in actuality, some the features of some matches will be reconstructed with higher errors than others. 

The features are encoded into a sparse representation and then decoded into the output layer.
This network architecture consists of a single hidden layer with sigmoid activation function and the output layer using
the identity activation function, but more complicated networks might have multiple layers of weights as the encoder and decoder.

The idea is that a sparse representation is enough to encode most matches, and because of this nature, 
the matches with higher reconstructon errors could be considered anomalous. 
Other uses of auto-encoders include dimension reduction or de-noising, but I will go along with the anomalous interpretation. 
Ideally, the most anomalous matches will have situations where some of the players feed or troll, network conditions are impacting 
numerous games but not enough to change the *standard* sparse represenations, or other weird stuff happens. 

The behavior, training, and tuning of auto-encoders is very similar to that of regular (not necessarily regularized) multi-layer perceptrons. 
Feed-forward and backpropagation are still key parts of the algorithm. The main constraint is that the final output layer needs to match the input layer as much as possible. 

Unfortunately, I had some issues with the data that were not forseen later, and there nothing beyond a rudimentary design in the neural
network, nonetheless, I found what are some interesting trends that corresponded 
to some of my prior intuitions, and, more excitingly, some trends completely contradicted my expectations. 
Because of this, I'd say that some more effort into one of the tasks I mention above could enable the
data (or a similar batch) into a more directed effort such. 

This data quality issues did impact the distribution of the 
residuals but still left a very faint pattern left. I failed to control over game type. Some of the matches were single draft. 
In fact, the first few times I tried this experiment, I was getting matches from the arcade mode **Dark Moon**.
I also first started this project a few months ago, so not all of the matches I initially collected were 
available on Dotabuff as I iterated on the features or the autoencoder architecture. 

A somewhat nuanced issue is the large number of matches were a substantial number of the players do not give third parties their match data.
In these games, a substantial number of players would not have some features accessible such as "APM" (which seems to have been removed in 
later versions of the OpenDota API).

Other issues in data arise from me collecting the wrong features or missing some features that could have been useful. 
I should have captured whether a player abandoned for example. The data has which heroes were selected for the particular roles---along with
their items---but the high cardinality of these dimensions made training infeasible on the machine I was using. 

There is still a lot of work to attempt in the architecture of the autoencoder. I attempted ReLu activation functions, but I was having
issues with dying neurons, especially in the iterations when I had multiple hidden layers. 

